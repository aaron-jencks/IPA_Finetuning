{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T18:56:26.211573Z",
     "start_time": "2025-07-30T18:56:26.209005Z"
    }
   },
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import wandb\n",
    "\n",
    "from hf_wrapper import GPTForSequenceClassification\n",
    "from tokenizer import load_tokenizer\n",
    "from utils import flatten_multi_features, load_random_from_pretrained_model, compute_metrics"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T18:56:26.285829Z",
     "start_time": "2025-07-30T18:56:26.251723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normal_checkpoint_location = pathlib.Path('./cache/checkpoints/russian_polish_normal_12_5_50k/ckpt.pt')\n",
    "ipa_checkpoint_location = pathlib.Path('./cache/checkpoints/russian_polish_ipa_12_5_50k/ckpt.pt')\n",
    "hf_cache = pathlib.Path('./cache')\n",
    "training_checkpoints = pathlib.Path('./cache/checkpoints')\n",
    "tokenizer_prefix = pathlib.Path('./cache/tokenizers')\n",
    "ipa_tokenizer_prefix = 'bpe-rus-pol-ipa-number-preservation'\n",
    "normal_tokenizer_prefix = 'bpe-rus-pol-normal-number-preservation'\n",
    "\n",
    "dataset_name = {\n",
    "    'rus': 'iggy12345/russian-xnli-ipa-rosetta',\n",
    "    'pol': 'iggy12345/cdsc-e-ipa-epitran'\n",
    "}\n",
    "\n",
    "epochs = 3\n",
    "context_size = 1024\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5"
   ],
   "id": "1bc0a742425c3e70",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T18:56:26.298348Z",
     "start_time": "2025-07-30T18:56:26.295418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_preprocess(lang: str, ipa: bool, split: str, tokenizer):\n",
    "    ds = load_dataset(dataset_name[lang], split=split, cache_dir=str(hf_cache))\n",
    "    column_names = ['hypothesis', 'premise']\n",
    "    if lang == 'pol':\n",
    "        column_names = ['sentence_A', 'sentence_B']\n",
    "    suffix = 'phoneme' if lang == 'pol' else 'epitran'\n",
    "    fields = [\n",
    "        f'{c}-{suffix}' if ipa else c\n",
    "        for c in column_names\n",
    "    ]\n",
    "\n",
    "    def preprocess(examples):\n",
    "        features = flatten_multi_features(examples, fields)\n",
    "        encoded = tokenizer(features, truncation=True, max_length=context_size)\n",
    "        encoded['label'] = examples['label']\n",
    "        return encoded\n",
    "\n",
    "    return ds.map(preprocess, batched=True, num_proc=os.cpu_count())"
   ],
   "id": "48af2659df9c52c9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T18:56:26.345651Z",
     "start_time": "2025-07-30T18:56:26.340123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(ipa: bool) -> Trainer:\n",
    "    checkpoint = ipa_checkpoint_location if ipa else normal_checkpoint_location\n",
    "\n",
    "    project_name = f\"debug-russian-polish-small-finetuning-xnli-random-initial-epitran\"\n",
    "    temporary_output_dir = training_checkpoints / f\"{project_name}-{'ipa' if ipa else 'normal'}/\"\n",
    "    temporary_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    vocab_path = tokenizer_prefix / f'{ipa_tokenizer_prefix if ipa else normal_tokenizer_prefix}-vocab.json'\n",
    "    merges_path = tokenizer_prefix / f'{ipa_tokenizer_prefix if ipa else normal_tokenizer_prefix}-merges.txt'\n",
    "    tokenizer = load_tokenizer(vocab_path, merges_path)\n",
    "\n",
    "    base_model = load_random_from_pretrained_model(checkpoint, 'cuda')\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    base_model.config.padding_side = tokenizer.padding_side\n",
    "    model = GPTForSequenceClassification(base_model, num_classes=3).to('cuda')\n",
    "\n",
    "    rus_train_dataset = load_and_preprocess('rus', ipa, 'train', tokenizer)\n",
    "    pol_train_dataset = load_and_preprocess('pol', ipa, 'train', tokenizer)\n",
    "    train_dataset = concatenate_datasets([rus_train_dataset, pol_train_dataset])\n",
    "\n",
    "    rus_eval_dataset = load_and_preprocess('rus', ipa, 'validation', tokenizer)\n",
    "    pol_eval_dataset = load_and_preprocess('pol', ipa, 'validation', tokenizer)\n",
    "    eval_dataset = concatenate_datasets([rus_eval_dataset, pol_eval_dataset])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "        output_dir=str(temporary_output_dir),\n",
    "        save_strategy='steps',\n",
    "        save_steps=1000,\n",
    "        metric_for_best_model=\"precision\",\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        fp16=True,\n",
    "        warmup_ratio=0.3,\n",
    "        save_safetensors=False,\n",
    "        # disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    wrun = wandb.init(entity='aaronjencks-the-ohio-state-university', project=project_name, name=f'{\"ipa\" if ipa else \"normal\"}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(f\"Training model\")\n",
    "    trainer.train()\n",
    "\n",
    "    wrun.finish()\n",
    "\n",
    "    return trainer"
   ],
   "id": "913188e9f0fea6ed",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T18:56:26.391139Z",
     "start_time": "2025-07-30T18:56:26.387440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def finetune_transcription(eval_lang: str, ipa: bool, model: Trainer):\n",
    "    vocab_path = tokenizer_prefix / f'{ipa_tokenizer_prefix if ipa else normal_tokenizer_prefix}-vocab.json'\n",
    "    merges_path = tokenizer_prefix / f'{ipa_tokenizer_prefix if ipa else normal_tokenizer_prefix}-merges.txt'\n",
    "    tokenizer = load_tokenizer(vocab_path, merges_path)\n",
    "\n",
    "    if eval_lang == 'both':\n",
    "        rus_eval_dataset = load_and_preprocess('rus', ipa, 'validation', tokenizer)\n",
    "        pol_eval_dataset = load_and_preprocess('pol', ipa, 'validation', tokenizer)\n",
    "        eval_dataset = concatenate_datasets([rus_eval_dataset, pol_eval_dataset])\n",
    "    else:\n",
    "        eval_dataset = load_and_preprocess(eval_lang, ipa, 'validation', tokenizer)\n",
    "\n",
    "    print(f\"Final evaluation on {eval_lang}\")\n",
    "    results = model.evaluate(eval_dataset=eval_dataset)\n",
    "    print(results)\n"
   ],
   "id": "4a1dd25261d02c14",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-30T18:56:26.442731Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_model(False)",
   "id": "ca6b0cd231af34c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.35M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▂▂▃▃▄▅▅▆▆▅▆▆▆▇▇▇▇▇▆▇▇█▇▇▇██▇█</td></tr><tr><td>eval/f1</td><td>▁▁▁▃▄▃▅▄▆▆▇▃▇▆▅▇▇▆▆█▅▇▇█▇▇▇██▇▇</td></tr><tr><td>eval/loss</td><td>█▇▆▅▅▅▄▅▃▄▂▅▂▃▃▂▂▃▃▂▃▂▂▁▂▂▃▁▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▁▂▂▃▃▃▄▅▅▆▇▆▆█▇▇▆▆▇▇▇▆▇▇▇▇█▇▇█</td></tr><tr><td>eval/recall</td><td>▁▁▂▂▃▃▄▅▅▆▆▅▆▆▆▇▇▇▇▇▆▇▇█▇▇▇██▇█</td></tr><tr><td>eval/runtime</td><td>▃▃▃▃▃▃▃▃▃▃▂▁▂▁▂▁▁▁▃▃▃▃▃▃▃▁▁▁▃▃█</td></tr><tr><td>eval/samples_per_second</td><td>▆▆▆▆▆▆▆▆▆▆▇█▇█▇███▆▆▆▆▆▆▆███▆▆▁</td></tr><tr><td>eval/steps_per_second</td><td>▆▆▆▆▆▆▆▆▆▆▇█▇█▇███▆▆▆▆▆▆▆███▆▆▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅▅▄▅▄▅▆▇▅▂▂▅▂▂▆▅▆▆▄▃▂▅▂▃█▆▃▇▃▁▃▆▃▇▂█▃▅▄▄</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇█████████▇▇▇▇</td></tr><tr><td>train/loss</td><td>██▇▆▇▆▆▅▅▅▄▅▄▄▃▃▂▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.5004</td></tr><tr><td>eval/f1</td><td>0.48737</td></tr><tr><td>eval/loss</td><td>0.98853</td></tr><tr><td>eval/precision</td><td>0.52566</td></tr><tr><td>eval/recall</td><td>0.5004</td></tr><tr><td>eval/runtime</td><td>3.3262</td></tr><tr><td>eval/samples_per_second</td><td>748.599</td></tr><tr><td>eval/steps_per_second</td><td>46.9</td></tr><tr><td>train/epoch</td><td>1.25379</td></tr><tr><td>train/global_step</td><td>31400</td></tr><tr><td>train/grad_norm</td><td>7.23919</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.9851</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rus-ipa</strong> at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/pqfmpuys' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/pqfmpuys</a><br> View project at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250730_135154-pqfmpuys/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/workspace/github/IPA_Finetuning/wandb/run-20250730_145642-dqr6vcn2</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/dqr6vcn2' target=\"_blank\">normal</a></strong> to <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/dqr6vcn2' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/dqr6vcn2</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16715/720901575.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='695' max='75132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  695/75132 00:28 < 51:48, 23.94 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for lang in ['rus', 'pol']:\n",
    "    finetune_transcription(lang, False, model)"
   ],
   "id": "ee41ff91877871a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T18:45:39.333434903Z",
     "start_time": "2025-07-30T02:35:04.791715Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_model(True)",
   "id": "814d5c667b22c408",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for lang in ['rus', 'pol']:\n",
    "    finetune_transcription(lang, True, model)"
   ],
   "id": "bb4d828982eb9e37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
