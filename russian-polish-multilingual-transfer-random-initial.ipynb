{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T18:56:26.211573Z",
     "start_time": "2025-07-30T18:56:26.209005Z"
    }
   },
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import wandb\n",
    "\n",
    "from hf_wrapper import GPTForSequenceClassification\n",
    "from tokenizer import load_tokenizer\n",
    "from utils import flatten_multi_features, load_random_from_pretrained_model, compute_metrics"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T18:56:26.285829Z",
     "start_time": "2025-07-30T18:56:26.251723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normal_checkpoint_location = pathlib.Path('./cache/checkpoints/russian_polish_normal_12_5_50k/ckpt.pt')\n",
    "ipa_checkpoint_location = pathlib.Path('./cache/checkpoints/russian_polish_ipa_12_5_50k/ckpt.pt')\n",
    "hf_cache = pathlib.Path('./cache')\n",
    "training_checkpoints = pathlib.Path('./cache/checkpoints')\n",
    "tokenizer_prefix = pathlib.Path('./cache/tokenizers')\n",
    "ipa_tokenizer_prefix = 'bpe-rus-pol-ipa-number-preservation'\n",
    "normal_tokenizer_prefix = 'bpe-rus-pol-normal-number-preservation'\n",
    "\n",
    "dataset_name = {\n",
    "    'rus': 'iggy12345/russian-xnli-ipa-rosetta',\n",
    "    'pol': 'iggy12345/cdsc-e-ipa-epitran'\n",
    "}\n",
    "\n",
    "epochs = 3\n",
    "context_size = 1024\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5"
   ],
   "id": "1bc0a742425c3e70",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T18:56:26.298348Z",
     "start_time": "2025-07-30T18:56:26.295418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_preprocess(lang: str, ipa: bool, split: str, tokenizer):\n",
    "    ds = load_dataset(dataset_name[lang], split=split, cache_dir=str(hf_cache))\n",
    "    column_names = ['hypothesis', 'premise']\n",
    "    if lang == 'pol':\n",
    "        column_names = ['sentence_A', 'sentence_B']\n",
    "    suffix = 'phoneme' if lang == 'pol' else 'epitran'\n",
    "    fields = [\n",
    "        f'{c}-{suffix}' if ipa else c\n",
    "        for c in column_names\n",
    "    ]\n",
    "\n",
    "    def preprocess(examples):\n",
    "        features = flatten_multi_features(examples, fields)\n",
    "        encoded = tokenizer(features, truncation=True, max_length=context_size)\n",
    "        encoded['label'] = examples['label']\n",
    "        return encoded\n",
    "\n",
    "    return ds.map(preprocess, batched=True, num_proc=os.cpu_count())"
   ],
   "id": "48af2659df9c52c9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:26:02.935065Z",
     "start_time": "2025-07-30T22:26:02.932933Z"
    }
   },
   "cell_type": "code",
   "source": "project_name = f\"debug-russian-polish-small-finetuning-xnli-random-initial-epitran\"",
   "id": "394283a0147aa759",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:26:06.778450Z",
     "start_time": "2025-07-30T22:26:06.774220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(ipa: bool) -> Trainer:\n",
    "    checkpoint = ipa_checkpoint_location if ipa else normal_checkpoint_location\n",
    "\n",
    "    temporary_output_dir = training_checkpoints / f\"{project_name}-{'ipa' if ipa else 'normal'}/\"\n",
    "    temporary_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    vocab_path = tokenizer_prefix / f'{ipa_tokenizer_prefix if ipa else normal_tokenizer_prefix}-vocab.json'\n",
    "    merges_path = tokenizer_prefix / f'{ipa_tokenizer_prefix if ipa else normal_tokenizer_prefix}-merges.txt'\n",
    "    tokenizer = load_tokenizer(vocab_path, merges_path)\n",
    "\n",
    "    base_model = load_random_from_pretrained_model(checkpoint, 'cuda')\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    base_model.config.padding_side = tokenizer.padding_side\n",
    "    model = GPTForSequenceClassification(base_model, num_classes=3).to('cuda')\n",
    "\n",
    "    rus_train_dataset = load_and_preprocess('rus', ipa, 'train', tokenizer)\n",
    "    pol_train_dataset = load_and_preprocess('pol', ipa, 'train', tokenizer)\n",
    "    train_dataset = concatenate_datasets([rus_train_dataset, pol_train_dataset])\n",
    "\n",
    "    rus_eval_dataset = load_and_preprocess('rus', ipa, 'validation', tokenizer)\n",
    "    pol_eval_dataset = load_and_preprocess('pol', ipa, 'validation', tokenizer)\n",
    "    eval_dataset = concatenate_datasets([rus_eval_dataset, pol_eval_dataset])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "        output_dir=str(temporary_output_dir),\n",
    "        save_strategy='steps',\n",
    "        save_steps=1000,\n",
    "        metric_for_best_model=\"precision\",\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        fp16=True,\n",
    "        warmup_ratio=0.3,\n",
    "        save_safetensors=False,\n",
    "        # disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    wrun = wandb.init(entity='aaronjencks-the-ohio-state-university', project=project_name, name=f'{\"ipa\" if ipa else \"normal\"}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(f\"Training model\")\n",
    "    trainer.train()\n",
    "\n",
    "    wrun.finish()\n",
    "\n",
    "    return trainer"
   ],
   "id": "913188e9f0fea6ed",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:30:29.066215Z",
     "start_time": "2025-07-30T22:30:29.063014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def finetune_transcription(eval_lang: str, ipa: bool, model: Trainer):\n",
    "    print('finetuning on {} {}'.format(eval_lang, 'ipa' if ipa else 'normal'))\n",
    "    vocab_path = tokenizer_prefix / f'{ipa_tokenizer_prefix if ipa else normal_tokenizer_prefix}-vocab.json'\n",
    "    merges_path = tokenizer_prefix / f'{ipa_tokenizer_prefix if ipa else normal_tokenizer_prefix}-merges.txt'\n",
    "    tokenizer = load_tokenizer(vocab_path, merges_path)\n",
    "\n",
    "    if eval_lang == 'both':\n",
    "        rus_eval_dataset = load_and_preprocess('rus', ipa, 'validation', tokenizer)\n",
    "        pol_eval_dataset = load_and_preprocess('pol', ipa, 'validation', tokenizer)\n",
    "        eval_dataset = concatenate_datasets([rus_eval_dataset, pol_eval_dataset])\n",
    "    else:\n",
    "        eval_dataset = load_and_preprocess(eval_lang, ipa, 'validation', tokenizer)\n",
    "\n",
    "    wrun = wandb.init(entity='aaronjencks-the-ohio-state-university', project=project_name, name=f'{eval_lang}-{\"ipa\" if ipa else \"normal\"}')\n",
    "\n",
    "    print(f\"Final evaluation on {eval_lang}\")\n",
    "    results = model.evaluate(eval_dataset=eval_dataset)\n",
    "    print(results)\n",
    "\n",
    "    wrun.finish()\n"
   ],
   "id": "4a1dd25261d02c14",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T19:54:31.594691Z",
     "start_time": "2025-07-30T18:56:26.442731Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_model(False)",
   "id": "ca6b0cd231af34c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.35M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▂▂▃▃▄▅▅▆▆▅▆▆▆▇▇▇▇▇▆▇▇█▇▇▇██▇█</td></tr><tr><td>eval/f1</td><td>▁▁▁▃▄▃▅▄▆▆▇▃▇▆▅▇▇▆▆█▅▇▇█▇▇▇██▇▇</td></tr><tr><td>eval/loss</td><td>█▇▆▅▅▅▄▅▃▄▂▅▂▃▃▂▂▃▃▂▃▂▂▁▂▂▃▁▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▁▂▂▃▃▃▄▅▅▆▇▆▆█▇▇▆▆▇▇▇▆▇▇▇▇█▇▇█</td></tr><tr><td>eval/recall</td><td>▁▁▂▂▃▃▄▅▅▆▆▅▆▆▆▇▇▇▇▇▆▇▇█▇▇▇██▇█</td></tr><tr><td>eval/runtime</td><td>▃▃▃▃▃▃▃▃▃▃▂▁▂▁▂▁▁▁▃▃▃▃▃▃▃▁▁▁▃▃█</td></tr><tr><td>eval/samples_per_second</td><td>▆▆▆▆▆▆▆▆▆▆▇█▇█▇███▆▆▆▆▆▆▆███▆▆▁</td></tr><tr><td>eval/steps_per_second</td><td>▆▆▆▆▆▆▆▆▆▆▇█▇█▇███▆▆▆▆▆▆▆███▆▆▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅▅▄▅▄▅▆▇▅▂▂▅▂▂▆▅▆▆▄▃▂▅▂▃█▆▃▇▃▁▃▆▃▇▂█▃▅▄▄</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇█████████▇▇▇▇</td></tr><tr><td>train/loss</td><td>██▇▆▇▆▆▅▅▅▄▅▄▄▃▃▂▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.5004</td></tr><tr><td>eval/f1</td><td>0.48737</td></tr><tr><td>eval/loss</td><td>0.98853</td></tr><tr><td>eval/precision</td><td>0.52566</td></tr><tr><td>eval/recall</td><td>0.5004</td></tr><tr><td>eval/runtime</td><td>3.3262</td></tr><tr><td>eval/samples_per_second</td><td>748.599</td></tr><tr><td>eval/steps_per_second</td><td>46.9</td></tr><tr><td>train/epoch</td><td>1.25379</td></tr><tr><td>train/global_step</td><td>31400</td></tr><tr><td>train/grad_norm</td><td>7.23919</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.9851</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rus-ipa</strong> at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/pqfmpuys' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/pqfmpuys</a><br> View project at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250730_135154-pqfmpuys/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/workspace/github/IPA_Finetuning/wandb/run-20250730_145642-dqr6vcn2</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/dqr6vcn2' target=\"_blank\">normal</a></strong> to <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/dqr6vcn2' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/dqr6vcn2</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16715/720901575.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75132' max='75132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75132/75132 57:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.116400</td>\n",
       "      <td>1.102437</td>\n",
       "      <td>0.353295</td>\n",
       "      <td>0.366824</td>\n",
       "      <td>0.353295</td>\n",
       "      <td>0.357517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>1.110719</td>\n",
       "      <td>0.418338</td>\n",
       "      <td>0.376648</td>\n",
       "      <td>0.418338</td>\n",
       "      <td>0.361614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.099500</td>\n",
       "      <td>1.052309</td>\n",
       "      <td>0.471060</td>\n",
       "      <td>0.428263</td>\n",
       "      <td>0.471060</td>\n",
       "      <td>0.378801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.090200</td>\n",
       "      <td>1.025270</td>\n",
       "      <td>0.495702</td>\n",
       "      <td>0.495177</td>\n",
       "      <td>0.495702</td>\n",
       "      <td>0.491580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.056800</td>\n",
       "      <td>1.012704</td>\n",
       "      <td>0.482521</td>\n",
       "      <td>0.497729</td>\n",
       "      <td>0.482521</td>\n",
       "      <td>0.471663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.061600</td>\n",
       "      <td>0.989607</td>\n",
       "      <td>0.519771</td>\n",
       "      <td>0.520287</td>\n",
       "      <td>0.519771</td>\n",
       "      <td>0.520007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.033900</td>\n",
       "      <td>0.971783</td>\n",
       "      <td>0.525215</td>\n",
       "      <td>0.511908</td>\n",
       "      <td>0.525215</td>\n",
       "      <td>0.514934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.026500</td>\n",
       "      <td>0.956076</td>\n",
       "      <td>0.540688</td>\n",
       "      <td>0.528632</td>\n",
       "      <td>0.540688</td>\n",
       "      <td>0.501979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.005200</td>\n",
       "      <td>0.943010</td>\n",
       "      <td>0.552722</td>\n",
       "      <td>0.560097</td>\n",
       "      <td>0.552722</td>\n",
       "      <td>0.555356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.007900</td>\n",
       "      <td>0.942268</td>\n",
       "      <td>0.556734</td>\n",
       "      <td>0.570033</td>\n",
       "      <td>0.556734</td>\n",
       "      <td>0.560552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.035400</td>\n",
       "      <td>0.937128</td>\n",
       "      <td>0.551289</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>0.551289</td>\n",
       "      <td>0.544229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.986400</td>\n",
       "      <td>0.978537</td>\n",
       "      <td>0.526074</td>\n",
       "      <td>0.621866</td>\n",
       "      <td>0.526074</td>\n",
       "      <td>0.529316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.015700</td>\n",
       "      <td>0.919594</td>\n",
       "      <td>0.570487</td>\n",
       "      <td>0.585091</td>\n",
       "      <td>0.570487</td>\n",
       "      <td>0.572496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.985400</td>\n",
       "      <td>0.929819</td>\n",
       "      <td>0.558453</td>\n",
       "      <td>0.564267</td>\n",
       "      <td>0.558453</td>\n",
       "      <td>0.513496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>0.939865</td>\n",
       "      <td>0.551289</td>\n",
       "      <td>0.611097</td>\n",
       "      <td>0.551289</td>\n",
       "      <td>0.557685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.005100</td>\n",
       "      <td>0.912668</td>\n",
       "      <td>0.570201</td>\n",
       "      <td>0.588550</td>\n",
       "      <td>0.570201</td>\n",
       "      <td>0.572889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.975600</td>\n",
       "      <td>0.928743</td>\n",
       "      <td>0.561605</td>\n",
       "      <td>0.570161</td>\n",
       "      <td>0.561605</td>\n",
       "      <td>0.538308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.004000</td>\n",
       "      <td>0.915627</td>\n",
       "      <td>0.562464</td>\n",
       "      <td>0.560696</td>\n",
       "      <td>0.562464</td>\n",
       "      <td>0.560990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.984600</td>\n",
       "      <td>0.935389</td>\n",
       "      <td>0.551289</td>\n",
       "      <td>0.558093</td>\n",
       "      <td>0.551289</td>\n",
       "      <td>0.504144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.908861</td>\n",
       "      <td>0.580516</td>\n",
       "      <td>0.587177</td>\n",
       "      <td>0.580516</td>\n",
       "      <td>0.576482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.979700</td>\n",
       "      <td>0.938115</td>\n",
       "      <td>0.558739</td>\n",
       "      <td>0.582964</td>\n",
       "      <td>0.558739</td>\n",
       "      <td>0.564483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.985800</td>\n",
       "      <td>0.911673</td>\n",
       "      <td>0.586533</td>\n",
       "      <td>0.591384</td>\n",
       "      <td>0.586533</td>\n",
       "      <td>0.577834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.982800</td>\n",
       "      <td>0.901106</td>\n",
       "      <td>0.586533</td>\n",
       "      <td>0.587630</td>\n",
       "      <td>0.586533</td>\n",
       "      <td>0.583346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>0.890368</td>\n",
       "      <td>0.585673</td>\n",
       "      <td>0.586507</td>\n",
       "      <td>0.585673</td>\n",
       "      <td>0.581775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.972700</td>\n",
       "      <td>0.910547</td>\n",
       "      <td>0.558739</td>\n",
       "      <td>0.571008</td>\n",
       "      <td>0.558739</td>\n",
       "      <td>0.503054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.969800</td>\n",
       "      <td>0.903171</td>\n",
       "      <td>0.582521</td>\n",
       "      <td>0.585048</td>\n",
       "      <td>0.582521</td>\n",
       "      <td>0.582393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.894471</td>\n",
       "      <td>0.584814</td>\n",
       "      <td>0.608207</td>\n",
       "      <td>0.584814</td>\n",
       "      <td>0.589923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.953400</td>\n",
       "      <td>0.886752</td>\n",
       "      <td>0.597135</td>\n",
       "      <td>0.612024</td>\n",
       "      <td>0.597135</td>\n",
       "      <td>0.599580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.983600</td>\n",
       "      <td>0.885065</td>\n",
       "      <td>0.590544</td>\n",
       "      <td>0.601268</td>\n",
       "      <td>0.590544</td>\n",
       "      <td>0.592950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.949300</td>\n",
       "      <td>0.882832</td>\n",
       "      <td>0.589398</td>\n",
       "      <td>0.624090</td>\n",
       "      <td>0.589398</td>\n",
       "      <td>0.594881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.967500</td>\n",
       "      <td>0.870783</td>\n",
       "      <td>0.593983</td>\n",
       "      <td>0.620924</td>\n",
       "      <td>0.593983</td>\n",
       "      <td>0.597036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.954400</td>\n",
       "      <td>0.872658</td>\n",
       "      <td>0.593123</td>\n",
       "      <td>0.609348</td>\n",
       "      <td>0.593123</td>\n",
       "      <td>0.595408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.942200</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.588252</td>\n",
       "      <td>0.581749</td>\n",
       "      <td>0.588252</td>\n",
       "      <td>0.580772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.961400</td>\n",
       "      <td>0.883980</td>\n",
       "      <td>0.580516</td>\n",
       "      <td>0.598523</td>\n",
       "      <td>0.580516</td>\n",
       "      <td>0.585407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.956800</td>\n",
       "      <td>0.869161</td>\n",
       "      <td>0.587393</td>\n",
       "      <td>0.581758</td>\n",
       "      <td>0.587393</td>\n",
       "      <td>0.572136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.940100</td>\n",
       "      <td>0.855749</td>\n",
       "      <td>0.598854</td>\n",
       "      <td>0.603402</td>\n",
       "      <td>0.598854</td>\n",
       "      <td>0.598226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.956900</td>\n",
       "      <td>0.858735</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.612138</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.599154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.946500</td>\n",
       "      <td>0.872395</td>\n",
       "      <td>0.581948</td>\n",
       "      <td>0.574059</td>\n",
       "      <td>0.581948</td>\n",
       "      <td>0.563320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>0.867122</td>\n",
       "      <td>0.599713</td>\n",
       "      <td>0.593674</td>\n",
       "      <td>0.599713</td>\n",
       "      <td>0.594784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.959100</td>\n",
       "      <td>0.850609</td>\n",
       "      <td>0.593983</td>\n",
       "      <td>0.635917</td>\n",
       "      <td>0.593983</td>\n",
       "      <td>0.599797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.850869</td>\n",
       "      <td>0.602865</td>\n",
       "      <td>0.609911</td>\n",
       "      <td>0.602865</td>\n",
       "      <td>0.605513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.851368</td>\n",
       "      <td>0.599713</td>\n",
       "      <td>0.617256</td>\n",
       "      <td>0.599713</td>\n",
       "      <td>0.604220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.832527</td>\n",
       "      <td>0.612894</td>\n",
       "      <td>0.624584</td>\n",
       "      <td>0.612894</td>\n",
       "      <td>0.615409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.924400</td>\n",
       "      <td>0.843226</td>\n",
       "      <td>0.606304</td>\n",
       "      <td>0.598303</td>\n",
       "      <td>0.606304</td>\n",
       "      <td>0.599020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.929900</td>\n",
       "      <td>0.820754</td>\n",
       "      <td>0.604871</td>\n",
       "      <td>0.611594</td>\n",
       "      <td>0.604871</td>\n",
       "      <td>0.607081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>0.829825</td>\n",
       "      <td>0.612894</td>\n",
       "      <td>0.624771</td>\n",
       "      <td>0.612894</td>\n",
       "      <td>0.616588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.922300</td>\n",
       "      <td>0.830890</td>\n",
       "      <td>0.604011</td>\n",
       "      <td>0.609386</td>\n",
       "      <td>0.604011</td>\n",
       "      <td>0.605127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.945700</td>\n",
       "      <td>0.818020</td>\n",
       "      <td>0.602292</td>\n",
       "      <td>0.604992</td>\n",
       "      <td>0.602292</td>\n",
       "      <td>0.603495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.918300</td>\n",
       "      <td>0.826174</td>\n",
       "      <td>0.600287</td>\n",
       "      <td>0.610198</td>\n",
       "      <td>0.600287</td>\n",
       "      <td>0.601518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.946100</td>\n",
       "      <td>0.831854</td>\n",
       "      <td>0.606590</td>\n",
       "      <td>0.618620</td>\n",
       "      <td>0.606590</td>\n",
       "      <td>0.609980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.876200</td>\n",
       "      <td>0.833466</td>\n",
       "      <td>0.604871</td>\n",
       "      <td>0.605588</td>\n",
       "      <td>0.604871</td>\n",
       "      <td>0.605129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.833292</td>\n",
       "      <td>0.606590</td>\n",
       "      <td>0.606437</td>\n",
       "      <td>0.606590</td>\n",
       "      <td>0.602781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.934100</td>\n",
       "      <td>0.817121</td>\n",
       "      <td>0.608023</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.608023</td>\n",
       "      <td>0.611319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.889300</td>\n",
       "      <td>0.822240</td>\n",
       "      <td>0.612321</td>\n",
       "      <td>0.631515</td>\n",
       "      <td>0.612321</td>\n",
       "      <td>0.617144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.834478</td>\n",
       "      <td>0.614900</td>\n",
       "      <td>0.618913</td>\n",
       "      <td>0.614900</td>\n",
       "      <td>0.616576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.919900</td>\n",
       "      <td>0.820057</td>\n",
       "      <td>0.612894</td>\n",
       "      <td>0.612027</td>\n",
       "      <td>0.612894</td>\n",
       "      <td>0.612104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>0.810175</td>\n",
       "      <td>0.622350</td>\n",
       "      <td>0.643601</td>\n",
       "      <td>0.622350</td>\n",
       "      <td>0.627422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.901500</td>\n",
       "      <td>0.815746</td>\n",
       "      <td>0.610029</td>\n",
       "      <td>0.635136</td>\n",
       "      <td>0.610029</td>\n",
       "      <td>0.615838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.893600</td>\n",
       "      <td>0.822116</td>\n",
       "      <td>0.600573</td>\n",
       "      <td>0.631790</td>\n",
       "      <td>0.600573</td>\n",
       "      <td>0.606167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.928700</td>\n",
       "      <td>0.805719</td>\n",
       "      <td>0.617479</td>\n",
       "      <td>0.636741</td>\n",
       "      <td>0.617479</td>\n",
       "      <td>0.622309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.911400</td>\n",
       "      <td>0.808533</td>\n",
       "      <td>0.612894</td>\n",
       "      <td>0.635733</td>\n",
       "      <td>0.612894</td>\n",
       "      <td>0.617939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.892300</td>\n",
       "      <td>0.811076</td>\n",
       "      <td>0.625501</td>\n",
       "      <td>0.628056</td>\n",
       "      <td>0.625501</td>\n",
       "      <td>0.626512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>0.808264</td>\n",
       "      <td>0.624642</td>\n",
       "      <td>0.631154</td>\n",
       "      <td>0.624642</td>\n",
       "      <td>0.626116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.881400</td>\n",
       "      <td>0.807508</td>\n",
       "      <td>0.620917</td>\n",
       "      <td>0.633296</td>\n",
       "      <td>0.620917</td>\n",
       "      <td>0.624517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.907200</td>\n",
       "      <td>0.809592</td>\n",
       "      <td>0.618052</td>\n",
       "      <td>0.638051</td>\n",
       "      <td>0.618052</td>\n",
       "      <td>0.622726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.896300</td>\n",
       "      <td>0.806744</td>\n",
       "      <td>0.621490</td>\n",
       "      <td>0.628987</td>\n",
       "      <td>0.621490</td>\n",
       "      <td>0.623979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.886900</td>\n",
       "      <td>0.813042</td>\n",
       "      <td>0.618338</td>\n",
       "      <td>0.630478</td>\n",
       "      <td>0.618338</td>\n",
       "      <td>0.621941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.883400</td>\n",
       "      <td>0.802191</td>\n",
       "      <td>0.622636</td>\n",
       "      <td>0.634183</td>\n",
       "      <td>0.622636</td>\n",
       "      <td>0.626220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.888800</td>\n",
       "      <td>0.803544</td>\n",
       "      <td>0.624642</td>\n",
       "      <td>0.636176</td>\n",
       "      <td>0.624642</td>\n",
       "      <td>0.628167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>0.801068</td>\n",
       "      <td>0.624928</td>\n",
       "      <td>0.631927</td>\n",
       "      <td>0.624928</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.904400</td>\n",
       "      <td>0.798339</td>\n",
       "      <td>0.618625</td>\n",
       "      <td>0.625047</td>\n",
       "      <td>0.618625</td>\n",
       "      <td>0.621066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.875300</td>\n",
       "      <td>0.798252</td>\n",
       "      <td>0.618625</td>\n",
       "      <td>0.628098</td>\n",
       "      <td>0.618625</td>\n",
       "      <td>0.621884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.804886</td>\n",
       "      <td>0.619771</td>\n",
       "      <td>0.630654</td>\n",
       "      <td>0.619771</td>\n",
       "      <td>0.623340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.887800</td>\n",
       "      <td>0.803273</td>\n",
       "      <td>0.619771</td>\n",
       "      <td>0.630353</td>\n",
       "      <td>0.619771</td>\n",
       "      <td>0.623268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.889800</td>\n",
       "      <td>0.803439</td>\n",
       "      <td>0.618338</td>\n",
       "      <td>0.626860</td>\n",
       "      <td>0.618338</td>\n",
       "      <td>0.621359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▄▄▅▆▆▅▇▆▇▆▇▇▇▇▇▇▇▇▇██▇▇█████▇█████████</td></tr><tr><td>eval/f1</td><td>▁▁▅▅▆▅▆▇▆▆▇▇▇▇▇▇▇▆▇▇▇█▇▇▇▇████▇█████████</td></tr><tr><td>eval/loss</td><td>█▇▆▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▄▄▅▅▆▇▇▆▆▆▇▆▇▇▇▇▇▆▇▇▇▇▇█▇▇▇█▇██████████</td></tr><tr><td>eval/recall</td><td>▁▄▅▄▅▆▆▆▅▆▇▆▇▆▇▇▆▇▇▇▇▇▇▇▇█▇▇██▇█████████</td></tr><tr><td>eval/runtime</td><td>▁▆▂▂▁▄▃▂▇▂▅▁▁▅▆█▇▅▅▅▅▅▅▅▅▅▆▇▅▅▅▅▄▅▅▅▅▅▅▅</td></tr><tr><td>eval/samples_per_second</td><td>▂▇▂█▅▆▇▁▅▅▄█▄▁▄▃▄▄▂▃▄▄▄▃▂▃▄▄▄▂▄▃▃▄▄▄▄▄▄▄</td></tr><tr><td>eval/steps_per_second</td><td>█▆█▂█▆▅▆▅▇▅▇▇██▄▄▃▄▄▁▄▂▄▄▄▄▄▄▃▄▄▂▄▄▄▄▄▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅▄▅█▄▆▂▅▂▂▃▂▂▅▂▇▃▃▁▃▃▂▄▄▄▂▃▅▃▂▃▂▃▅▂▂▃▅▂▄</td></tr><tr><td>train/learning_rate</td><td>▂▃▃▄▄▆▆▇▇██▇▇▇▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▅▅▄▄▅▄▅▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.61834</td></tr><tr><td>eval/f1</td><td>0.62136</td></tr><tr><td>eval/loss</td><td>0.80344</td></tr><tr><td>eval/precision</td><td>0.62686</td></tr><tr><td>eval/recall</td><td>0.61834</td></tr><tr><td>eval/runtime</td><td>1.7301</td></tr><tr><td>eval/samples_per_second</td><td>2017.191</td></tr><tr><td>eval/steps_per_second</td><td>126.58</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>75132</td></tr><tr><td>train/grad_norm</td><td>8.4293</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8584</td></tr><tr><td>train_loss</td><td>0.95524</td></tr><tr><td>train_runtime</td><td>3433.6911</td></tr><tr><td>train_samples_per_second</td><td>350.091</td></tr><tr><td>train_steps_per_second</td><td>21.881</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">normal</strong> at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/dqr6vcn2' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/dqr6vcn2</a><br> View project at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250730_145642-dqr6vcn2/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:31:12.635118Z",
     "start_time": "2025-07-30T22:31:06.276028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for lang in ['rus', 'pol']:\n",
    "    finetune_transcription(lang, False, model)"
   ],
   "id": "ee41ff91877871a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning on rus normal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/workspace/github/IPA_Finetuning/wandb/run-20250730_183107-ra28icyu</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/ra28icyu' target=\"_blank\">rus-normal</a></strong> to <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/ra28icyu' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/ra28icyu</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation on rus\n",
      "{'eval_loss': 0.940982460975647, 'eval_accuracy': 0.5582329317269076, 'eval_precision': 0.583023206611427, 'eval_recall': 0.5582329317269076, 'eval_f1': 0.5521867437545983, 'eval_runtime': 1.2705, 'eval_samples_per_second': 1959.805, 'eval_steps_per_second': 122.783, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.55823</td></tr><tr><td>eval/f1</td><td>0.55219</td></tr><tr><td>eval/loss</td><td>0.94098</td></tr><tr><td>eval/precision</td><td>0.58302</td></tr><tr><td>eval/recall</td><td>0.55823</td></tr><tr><td>eval/runtime</td><td>1.2705</td></tr><tr><td>eval/samples_per_second</td><td>1959.805</td></tr><tr><td>eval/steps_per_second</td><td>122.783</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>75132</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rus-normal</strong> at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/ra28icyu' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/ra28icyu</a><br> View project at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250730_183107-ra28icyu/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning on pol normal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/workspace/github/IPA_Finetuning/wandb/run-20250730_183110-muz75cd8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/muz75cd8' target=\"_blank\">pol-normal</a></strong> to <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/muz75cd8' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/muz75cd8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation on pol\n",
      "{'eval_loss': 0.48446381092071533, 'eval_accuracy': 0.782, 'eval_precision': 0.7528668352195749, 'eval_recall': 0.782, 'eval_f1': 0.7515182573847186, 'eval_runtime': 0.5379, 'eval_samples_per_second': 1859.147, 'eval_steps_per_second': 117.126, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.782</td></tr><tr><td>eval/f1</td><td>0.75152</td></tr><tr><td>eval/loss</td><td>0.48446</td></tr><tr><td>eval/precision</td><td>0.75287</td></tr><tr><td>eval/recall</td><td>0.782</td></tr><tr><td>eval/runtime</td><td>0.5379</td></tr><tr><td>eval/samples_per_second</td><td>1859.147</td></tr><tr><td>eval/steps_per_second</td><td>117.126</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>75132</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pol-normal</strong> at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/muz75cd8' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/muz75cd8</a><br> View project at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250730_183110-muz75cd8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:33:22.214298Z",
     "start_time": "2025-07-30T22:31:42.107881Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_model(True)",
   "id": "814d5c667b22c408",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.35M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/workspace/github/IPA_Finetuning/wandb/run-20250730_183155-0zrkj2i5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/0zrkj2i5' target=\"_blank\">ipa</a></strong> to <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/0zrkj2i5' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/0zrkj2i5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16715/4183944142.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75132' max='75132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75132/75132 2:01:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.120900</td>\n",
       "      <td>1.123244</td>\n",
       "      <td>0.383668</td>\n",
       "      <td>0.363487</td>\n",
       "      <td>0.383668</td>\n",
       "      <td>0.362575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.104500</td>\n",
       "      <td>1.076266</td>\n",
       "      <td>0.451862</td>\n",
       "      <td>0.399139</td>\n",
       "      <td>0.451862</td>\n",
       "      <td>0.386382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.109500</td>\n",
       "      <td>1.032806</td>\n",
       "      <td>0.457307</td>\n",
       "      <td>0.400172</td>\n",
       "      <td>0.457307</td>\n",
       "      <td>0.352088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.095100</td>\n",
       "      <td>1.003933</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>0.445404</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>0.416336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.078000</td>\n",
       "      <td>0.994341</td>\n",
       "      <td>0.485673</td>\n",
       "      <td>0.460979</td>\n",
       "      <td>0.485673</td>\n",
       "      <td>0.454481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.082400</td>\n",
       "      <td>0.990441</td>\n",
       "      <td>0.494269</td>\n",
       "      <td>0.502709</td>\n",
       "      <td>0.494269</td>\n",
       "      <td>0.470309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.067300</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.498567</td>\n",
       "      <td>0.495594</td>\n",
       "      <td>0.498567</td>\n",
       "      <td>0.490027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.059500</td>\n",
       "      <td>0.990714</td>\n",
       "      <td>0.510029</td>\n",
       "      <td>0.466999</td>\n",
       "      <td>0.510029</td>\n",
       "      <td>0.427499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.040500</td>\n",
       "      <td>0.956619</td>\n",
       "      <td>0.522636</td>\n",
       "      <td>0.521559</td>\n",
       "      <td>0.522636</td>\n",
       "      <td>0.521578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.027400</td>\n",
       "      <td>0.965498</td>\n",
       "      <td>0.520344</td>\n",
       "      <td>0.525837</td>\n",
       "      <td>0.520344</td>\n",
       "      <td>0.502028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.043300</td>\n",
       "      <td>0.937504</td>\n",
       "      <td>0.540115</td>\n",
       "      <td>0.534867</td>\n",
       "      <td>0.540115</td>\n",
       "      <td>0.536906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.015800</td>\n",
       "      <td>0.978111</td>\n",
       "      <td>0.521777</td>\n",
       "      <td>0.598732</td>\n",
       "      <td>0.521777</td>\n",
       "      <td>0.526399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.030700</td>\n",
       "      <td>0.935032</td>\n",
       "      <td>0.546132</td>\n",
       "      <td>0.550343</td>\n",
       "      <td>0.546132</td>\n",
       "      <td>0.547212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.024900</td>\n",
       "      <td>0.941736</td>\n",
       "      <td>0.540974</td>\n",
       "      <td>0.533409</td>\n",
       "      <td>0.540974</td>\n",
       "      <td>0.487923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.011900</td>\n",
       "      <td>0.959853</td>\n",
       "      <td>0.543553</td>\n",
       "      <td>0.598820</td>\n",
       "      <td>0.543553</td>\n",
       "      <td>0.550472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.015100</td>\n",
       "      <td>0.936794</td>\n",
       "      <td>0.553582</td>\n",
       "      <td>0.589631</td>\n",
       "      <td>0.553582</td>\n",
       "      <td>0.557081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.939108</td>\n",
       "      <td>0.552436</td>\n",
       "      <td>0.544813</td>\n",
       "      <td>0.552436</td>\n",
       "      <td>0.526517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.020400</td>\n",
       "      <td>0.935330</td>\n",
       "      <td>0.556447</td>\n",
       "      <td>0.575799</td>\n",
       "      <td>0.556447</td>\n",
       "      <td>0.561970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.005900</td>\n",
       "      <td>0.963097</td>\n",
       "      <td>0.537536</td>\n",
       "      <td>0.528263</td>\n",
       "      <td>0.537536</td>\n",
       "      <td>0.508804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.000400</td>\n",
       "      <td>0.929251</td>\n",
       "      <td>0.557307</td>\n",
       "      <td>0.551915</td>\n",
       "      <td>0.557307</td>\n",
       "      <td>0.552851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.016900</td>\n",
       "      <td>0.957596</td>\n",
       "      <td>0.538682</td>\n",
       "      <td>0.562531</td>\n",
       "      <td>0.538682</td>\n",
       "      <td>0.543788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.005100</td>\n",
       "      <td>0.984183</td>\n",
       "      <td>0.563037</td>\n",
       "      <td>0.561959</td>\n",
       "      <td>0.563037</td>\n",
       "      <td>0.553985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.017400</td>\n",
       "      <td>0.934015</td>\n",
       "      <td>0.551576</td>\n",
       "      <td>0.537911</td>\n",
       "      <td>0.551576</td>\n",
       "      <td>0.537542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.988100</td>\n",
       "      <td>0.918325</td>\n",
       "      <td>0.557307</td>\n",
       "      <td>0.547661</td>\n",
       "      <td>0.557307</td>\n",
       "      <td>0.549363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.006400</td>\n",
       "      <td>0.920789</td>\n",
       "      <td>0.546991</td>\n",
       "      <td>0.536583</td>\n",
       "      <td>0.546991</td>\n",
       "      <td>0.502617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.981300</td>\n",
       "      <td>0.922364</td>\n",
       "      <td>0.560458</td>\n",
       "      <td>0.575669</td>\n",
       "      <td>0.560458</td>\n",
       "      <td>0.564901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>0.937383</td>\n",
       "      <td>0.555587</td>\n",
       "      <td>0.590492</td>\n",
       "      <td>0.555587</td>\n",
       "      <td>0.562557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.977200</td>\n",
       "      <td>0.916500</td>\n",
       "      <td>0.570774</td>\n",
       "      <td>0.569135</td>\n",
       "      <td>0.570774</td>\n",
       "      <td>0.569822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.010500</td>\n",
       "      <td>0.914229</td>\n",
       "      <td>0.562464</td>\n",
       "      <td>0.553113</td>\n",
       "      <td>0.562464</td>\n",
       "      <td>0.554903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.964500</td>\n",
       "      <td>0.939762</td>\n",
       "      <td>0.560172</td>\n",
       "      <td>0.573160</td>\n",
       "      <td>0.560172</td>\n",
       "      <td>0.564686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.983000</td>\n",
       "      <td>0.899861</td>\n",
       "      <td>0.578223</td>\n",
       "      <td>0.596092</td>\n",
       "      <td>0.578223</td>\n",
       "      <td>0.580181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.966500</td>\n",
       "      <td>0.899435</td>\n",
       "      <td>0.567335</td>\n",
       "      <td>0.569371</td>\n",
       "      <td>0.567335</td>\n",
       "      <td>0.568261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.962700</td>\n",
       "      <td>0.903443</td>\n",
       "      <td>0.563037</td>\n",
       "      <td>0.550474</td>\n",
       "      <td>0.563037</td>\n",
       "      <td>0.548206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.979900</td>\n",
       "      <td>0.896908</td>\n",
       "      <td>0.571633</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.571633</td>\n",
       "      <td>0.572726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.888991</td>\n",
       "      <td>0.574499</td>\n",
       "      <td>0.568842</td>\n",
       "      <td>0.574499</td>\n",
       "      <td>0.562133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.967600</td>\n",
       "      <td>0.881141</td>\n",
       "      <td>0.591404</td>\n",
       "      <td>0.591062</td>\n",
       "      <td>0.591404</td>\n",
       "      <td>0.585086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.880618</td>\n",
       "      <td>0.579943</td>\n",
       "      <td>0.585956</td>\n",
       "      <td>0.579943</td>\n",
       "      <td>0.582239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>0.914249</td>\n",
       "      <td>0.550430</td>\n",
       "      <td>0.557118</td>\n",
       "      <td>0.550430</td>\n",
       "      <td>0.531758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.948800</td>\n",
       "      <td>0.912828</td>\n",
       "      <td>0.572779</td>\n",
       "      <td>0.559417</td>\n",
       "      <td>0.572779</td>\n",
       "      <td>0.539369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.969400</td>\n",
       "      <td>0.879132</td>\n",
       "      <td>0.585387</td>\n",
       "      <td>0.608514</td>\n",
       "      <td>0.585387</td>\n",
       "      <td>0.588245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.965400</td>\n",
       "      <td>0.878437</td>\n",
       "      <td>0.585673</td>\n",
       "      <td>0.583634</td>\n",
       "      <td>0.585673</td>\n",
       "      <td>0.584545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.882781</td>\n",
       "      <td>0.586819</td>\n",
       "      <td>0.587603</td>\n",
       "      <td>0.586819</td>\n",
       "      <td>0.587193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.943600</td>\n",
       "      <td>0.872147</td>\n",
       "      <td>0.588539</td>\n",
       "      <td>0.596515</td>\n",
       "      <td>0.588539</td>\n",
       "      <td>0.589105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.899029</td>\n",
       "      <td>0.580802</td>\n",
       "      <td>0.573266</td>\n",
       "      <td>0.580802</td>\n",
       "      <td>0.561784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.951900</td>\n",
       "      <td>0.878227</td>\n",
       "      <td>0.574499</td>\n",
       "      <td>0.572267</td>\n",
       "      <td>0.574499</td>\n",
       "      <td>0.566320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.930200</td>\n",
       "      <td>0.889031</td>\n",
       "      <td>0.574499</td>\n",
       "      <td>0.575771</td>\n",
       "      <td>0.574499</td>\n",
       "      <td>0.563282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.933400</td>\n",
       "      <td>0.876271</td>\n",
       "      <td>0.589112</td>\n",
       "      <td>0.583608</td>\n",
       "      <td>0.589112</td>\n",
       "      <td>0.583458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.951800</td>\n",
       "      <td>0.873724</td>\n",
       "      <td>0.586819</td>\n",
       "      <td>0.588369</td>\n",
       "      <td>0.586819</td>\n",
       "      <td>0.587497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.934300</td>\n",
       "      <td>0.864098</td>\n",
       "      <td>0.600573</td>\n",
       "      <td>0.604373</td>\n",
       "      <td>0.600573</td>\n",
       "      <td>0.602102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.964800</td>\n",
       "      <td>0.873958</td>\n",
       "      <td>0.586533</td>\n",
       "      <td>0.590243</td>\n",
       "      <td>0.586533</td>\n",
       "      <td>0.587156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.917300</td>\n",
       "      <td>0.875638</td>\n",
       "      <td>0.589398</td>\n",
       "      <td>0.580166</td>\n",
       "      <td>0.589398</td>\n",
       "      <td>0.577381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.926500</td>\n",
       "      <td>0.871411</td>\n",
       "      <td>0.597994</td>\n",
       "      <td>0.591336</td>\n",
       "      <td>0.597994</td>\n",
       "      <td>0.592289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.950300</td>\n",
       "      <td>0.865003</td>\n",
       "      <td>0.597708</td>\n",
       "      <td>0.600917</td>\n",
       "      <td>0.597708</td>\n",
       "      <td>0.598780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.913900</td>\n",
       "      <td>0.862333</td>\n",
       "      <td>0.595989</td>\n",
       "      <td>0.604033</td>\n",
       "      <td>0.595989</td>\n",
       "      <td>0.598770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>0.885120</td>\n",
       "      <td>0.588252</td>\n",
       "      <td>0.590379</td>\n",
       "      <td>0.588252</td>\n",
       "      <td>0.588937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.855646</td>\n",
       "      <td>0.606017</td>\n",
       "      <td>0.605757</td>\n",
       "      <td>0.606017</td>\n",
       "      <td>0.605433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.915100</td>\n",
       "      <td>0.863722</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.618398</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.604678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.925100</td>\n",
       "      <td>0.858853</td>\n",
       "      <td>0.600573</td>\n",
       "      <td>0.607400</td>\n",
       "      <td>0.600573</td>\n",
       "      <td>0.602841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.919400</td>\n",
       "      <td>0.861199</td>\n",
       "      <td>0.598854</td>\n",
       "      <td>0.615552</td>\n",
       "      <td>0.598854</td>\n",
       "      <td>0.603278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.941700</td>\n",
       "      <td>0.848664</td>\n",
       "      <td>0.607450</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.607450</td>\n",
       "      <td>0.609928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.851048</td>\n",
       "      <td>0.602865</td>\n",
       "      <td>0.613342</td>\n",
       "      <td>0.602865</td>\n",
       "      <td>0.605354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.938300</td>\n",
       "      <td>0.850606</td>\n",
       "      <td>0.599427</td>\n",
       "      <td>0.599617</td>\n",
       "      <td>0.599427</td>\n",
       "      <td>0.599160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.904500</td>\n",
       "      <td>0.847662</td>\n",
       "      <td>0.603725</td>\n",
       "      <td>0.603095</td>\n",
       "      <td>0.603725</td>\n",
       "      <td>0.600993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.904300</td>\n",
       "      <td>0.858907</td>\n",
       "      <td>0.594269</td>\n",
       "      <td>0.613439</td>\n",
       "      <td>0.594269</td>\n",
       "      <td>0.598994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.849890</td>\n",
       "      <td>0.606877</td>\n",
       "      <td>0.615142</td>\n",
       "      <td>0.606877</td>\n",
       "      <td>0.609635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.921600</td>\n",
       "      <td>0.856147</td>\n",
       "      <td>0.599427</td>\n",
       "      <td>0.602698</td>\n",
       "      <td>0.599427</td>\n",
       "      <td>0.600500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.855078</td>\n",
       "      <td>0.597708</td>\n",
       "      <td>0.600427</td>\n",
       "      <td>0.597708</td>\n",
       "      <td>0.597766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.896500</td>\n",
       "      <td>0.850951</td>\n",
       "      <td>0.604298</td>\n",
       "      <td>0.612883</td>\n",
       "      <td>0.604298</td>\n",
       "      <td>0.607224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.924200</td>\n",
       "      <td>0.847094</td>\n",
       "      <td>0.604298</td>\n",
       "      <td>0.604303</td>\n",
       "      <td>0.604298</td>\n",
       "      <td>0.604286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>0.845553</td>\n",
       "      <td>0.606877</td>\n",
       "      <td>0.607815</td>\n",
       "      <td>0.606877</td>\n",
       "      <td>0.607277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.916100</td>\n",
       "      <td>0.844119</td>\n",
       "      <td>0.604011</td>\n",
       "      <td>0.604108</td>\n",
       "      <td>0.604011</td>\n",
       "      <td>0.604020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.883300</td>\n",
       "      <td>0.847131</td>\n",
       "      <td>0.606877</td>\n",
       "      <td>0.608080</td>\n",
       "      <td>0.606877</td>\n",
       "      <td>0.607442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.854056</td>\n",
       "      <td>0.602006</td>\n",
       "      <td>0.606164</td>\n",
       "      <td>0.602006</td>\n",
       "      <td>0.602962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.885400</td>\n",
       "      <td>0.850376</td>\n",
       "      <td>0.604011</td>\n",
       "      <td>0.609907</td>\n",
       "      <td>0.604011</td>\n",
       "      <td>0.605863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.919500</td>\n",
       "      <td>0.849362</td>\n",
       "      <td>0.602865</td>\n",
       "      <td>0.606086</td>\n",
       "      <td>0.602865</td>\n",
       "      <td>0.603848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▄▄▅▆▆▆▆▆▇▆▆▆▇▇▇█▇▇▇▇█▇▇█▇█████████████</td></tr><tr><td>eval/f1</td><td>▁▂▃▄▄▅▆▆▆▅▆▆▆▇▆▇▆▇▇▇▆▇▇▇▇█▇▇████████████</td></tr><tr><td>eval/loss</td><td>█▇▄▃▄▃▃▄▅▃▃▃▃▃▃▂▂▂▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▂▄▃▅▇▆▅▆▇▆▅▆▅▇▆▆▇▆▇▇▇▇▇█▇▇██████▇█▇▇███</td></tr><tr><td>eval/recall</td><td>▁▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇███████████</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▁▁█▃▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▂▃▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>██▇██▇▁▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▄▄▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>eval/steps_per_second</td><td>███▇▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▅▆▅▆▆▇▇▇▇▇▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▅▅▂█▂▂▂▅▇▆▂▃▃▄▁▃▆▃▄▂▅▄▆▄▂▄▁▆▂▃▅▄▆▃▅▅▂▃▄▃</td></tr><tr><td>train/learning_rate</td><td>▂▂▃▃▃▃▃▄▄▅▆▆▆▇▇█▇▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▆▆▅▅▆▄▄▅▅▅▅▄▃▄▄▃▃▄▃▃▃▂▂▃▃▂▂▃▃▃▂▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.60287</td></tr><tr><td>eval/f1</td><td>0.60385</td></tr><tr><td>eval/loss</td><td>0.84936</td></tr><tr><td>eval/precision</td><td>0.60609</td></tr><tr><td>eval/recall</td><td>0.60287</td></tr><tr><td>eval/runtime</td><td>4.2991</td></tr><tr><td>eval/samples_per_second</td><td>811.804</td></tr><tr><td>eval/steps_per_second</td><td>50.941</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>75132</td></tr><tr><td>train/grad_norm</td><td>7.26349</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8859</td></tr><tr><td>train_loss</td><td>0.97525</td></tr><tr><td>train_runtime</td><td>7284.5104</td></tr><tr><td>train_samples_per_second</td><td>165.022</td></tr><tr><td>train_steps_per_second</td><td>10.314</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ipa</strong> at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/0zrkj2i5' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/0zrkj2i5</a><br> View project at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250730_183155-0zrkj2i5/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:33:31.706213Z",
     "start_time": "2025-07-31T00:33:22.225657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for lang in ['rus', 'pol']:\n",
    "    finetune_transcription(lang, True, model)"
   ],
   "id": "bb4d828982eb9e37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning on rus ipa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/workspace/github/IPA_Finetuning/wandb/run-20250730_203323-podelwtv</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/podelwtv' target=\"_blank\">rus-ipa</a></strong> to <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/podelwtv' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/podelwtv</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation on rus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 00:02]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9713320136070251, 'eval_accuracy': 0.5325301204819277, 'eval_precision': 0.5493527148366735, 'eval_recall': 0.5325301204819277, 'eval_f1': 0.5193151602512706, 'eval_runtime': 2.8267, 'eval_samples_per_second': 880.894, 'eval_steps_per_second': 55.189, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.53253</td></tr><tr><td>eval/f1</td><td>0.51932</td></tr><tr><td>eval/loss</td><td>0.97133</td></tr><tr><td>eval/precision</td><td>0.54935</td></tr><tr><td>eval/recall</td><td>0.53253</td></tr><tr><td>eval/runtime</td><td>2.8267</td></tr><tr><td>eval/samples_per_second</td><td>880.894</td></tr><tr><td>eval/steps_per_second</td><td>55.189</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>75132</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rus-ipa</strong> at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/podelwtv' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/podelwtv</a><br> View project at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250730_203323-podelwtv/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning on pol ipa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/workspace/github/IPA_Finetuning/wandb/run-20250730_203329-xp30b1y4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/xp30b1y4' target=\"_blank\">pol-ipa</a></strong> to <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 00:06]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation on pol\n",
      "{'eval_loss': 0.595771849155426, 'eval_accuracy': 0.768, 'eval_precision': 0.7431457140546661, 'eval_recall': 0.768, 'eval_f1': 0.7199687680338691, 'eval_runtime': 1.4132, 'eval_samples_per_second': 707.629, 'eval_steps_per_second': 44.581, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.768</td></tr><tr><td>eval/f1</td><td>0.71997</td></tr><tr><td>eval/loss</td><td>0.59577</td></tr><tr><td>eval/precision</td><td>0.74315</td></tr><tr><td>eval/recall</td><td>0.768</td></tr><tr><td>eval/runtime</td><td>1.4132</td></tr><tr><td>eval/samples_per_second</td><td>707.629</td></tr><tr><td>eval/steps_per_second</td><td>44.581</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>75132</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pol-ipa</strong> at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/xp30b1y4' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran/runs/xp30b1y4</a><br> View project at: <a href='https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran' target=\"_blank\">https://wandb.ai/aaronjencks-the-ohio-state-university/debug-russian-polish-small-finetuning-xnli-random-initial-epitran</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250730_203329-xp30b1y4/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f74404dab6f22174"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
